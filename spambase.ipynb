{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_validate, GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prep(data):\n",
    "    # Load and preprocess the dataset\n",
    "\n",
    "    # Split the dataset into training and testing sets\n",
    "    X = data.iloc[:, :-1]\n",
    "    y = data[\"label\"]\n",
    "    # Apply feature scaling\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3.42433707e-01  3.30884903e-01  7.12858774e-01 ... -4.52472762e-02\n",
      "   4.52979198e-02 -8.72413388e-03]\n",
      " [ 3.45359395e-01  5.19091945e-02  4.35129540e-01 ... -2.44326749e-03\n",
      "   2.50562832e-01  1.22832407e+00]\n",
      " [-1.45921392e-01 -1.65071912e-01  8.51723390e-01 ...  1.45920848e-01\n",
      "   2.22110599e+00  3.25873251e+00]\n",
      " ...\n",
      " [ 6.40127868e-01 -1.65071912e-01  3.83734930e-02 ... -1.19382054e-01\n",
      "  -2.36941335e-01 -2.72627750e-01]\n",
      " [ 2.80176333e+00 -1.65071912e-01 -5.56760578e-01 ... -1.27482666e-01\n",
      "  -2.42072958e-01 -3.38603654e-01]\n",
      " [-3.42433707e-01 -1.65071912e-01  7.32696576e-01 ... -1.24236117e-01\n",
      "  -2.42072958e-01 -4.01280763e-01]]\n",
      "0       1\n",
      "1       1\n",
      "2       1\n",
      "3       1\n",
      "4       1\n",
      "       ..\n",
      "4596    0\n",
      "4597    0\n",
      "4598    0\n",
      "4599    0\n",
      "4600    0\n",
      "Name: label, Length: 4601, dtype: int64\n",
      "1813 2788\n"
     ]
    }
   ],
   "source": [
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data\"\n",
    "column_names = [f\"attribute_{i}\" for i in range(1, 58)] + [\"label\"]\n",
    "df = pd.read_csv(url, header=None, names=column_names)\n",
    "# Split the dataset into training and testing sets\n",
    "X = df.iloc[:, :-1]\n",
    "y = df[\"label\"]\n",
    "# Apply feature scaling\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "print(X)\n",
    "print(y)\n",
    "s=0\n",
    "k = 0\n",
    "for i in y:\n",
    "    if i ==1:\n",
    "        s+=1\n",
    "    else:\n",
    "        k+=1\n",
    "print(s,k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base Models\n",
    "def base_models(X, y, scoring=\"roc_auc\"):\n",
    "    print(\"Base Models....\")\n",
    "    classifiers = [('LR', LogisticRegression()),\n",
    "                   ('KNN', KNeighborsClassifier()),\n",
    "                   (\"SVC\", SVC()),\n",
    "                   (\"CART\", DecisionTreeClassifier()),\n",
    "                   (\"RF\", RandomForestClassifier()),\n",
    "                   ('Adaboost', AdaBoostClassifier()),\n",
    "                   ('GBM', GradientBoostingClassifier()),\n",
    "                   ('LightGBM', LGBMClassifier()),\n",
    "                   # ('CatBoost', CatBoostClassifier(verbose=False))\n",
    "                   ]\n",
    "\n",
    "    for name, classifier in classifiers:\n",
    "        cv_results = cross_validate(classifier, X, y, cv=3, scoring=scoring)\n",
    "        print(f\"{scoring}: {round(cv_results['test_score'].mean(), 4)} ({name}) \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_params = {\"n_neighbors\": [1, 3, 5, 7, 9]}\n",
    "              #\"metric\":[1, 2]}\n",
    "\n",
    "\n",
    "cart_params = { \"min_samples_leaf\": [1, 2, 5, 10, 20],\n",
    "    \"ccp_alpha\": [0],  # Complexity parameter set to zero\n",
    "    \"min_samples_split\": [2, 4, 10, 20, 40]}\n",
    "\n",
    "rf_params = {\"n_estimators\": [500],  # J=500\n",
    "    \"max_features\": ['sqrt', 'log2', None],  # Choose appropriate value for \"m\"\n",
    "    \"min_samples_leaf\": [5]}\n",
    "\n",
    "gbm_params = { \"max_depth\": [3, 4, 5],  # Depth\n",
    "    \"n_estimators\": [50, 100, 150, 200, 250],  # Number of Trees\n",
    "    \"learning_rate\": [0.01, 0.1]}\n",
    "\n",
    "classifiers = [('KNN', KNeighborsClassifier(), knn_params),\n",
    "               (\"CART\", DecisionTreeClassifier(), cart_params),\n",
    "               (\"RF\", RandomForestClassifier(), rf_params),\n",
    "               ('GBM', GradientBoostingClassifier(), gbm_params)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter_optimization(X, y, cv=3, scoring=\"roc_auc\"):\n",
    "    print(\"Hyperparameter Optimization....\")\n",
    "    best_models = {}\n",
    "    for name, classifier, params in classifiers:\n",
    "        print(f\"########## {name} ##########\")\n",
    "        cv_results = cross_validate(classifier, X, y, cv=cv, scoring=scoring)\n",
    "        print(f\"{scoring} (Before): {round(cv_results['test_score'].mean(), 4)}\")\n",
    "\n",
    "        gs_best = GridSearchCV(classifier, params, cv=cv, n_jobs=-1, verbose=False).fit(X, y)\n",
    "        final_model = classifier.set_params(**gs_best.best_params_)\n",
    "\n",
    "        cv_results = cross_validate(final_model, X, y, cv=cv, scoring=scoring)\n",
    "        print(f\"{scoring} (After): {round(cv_results['test_score'].mean(), 4)}\")\n",
    "        print(f\"{name} best params: {gs_best.best_params_}\", end=\"\\n\\n\")\n",
    "        best_models[name] = final_model\n",
    "    return best_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def voting_classifier(best_models, X, y):\n",
    "    print(\"Voting Classifier...\")\n",
    "    voting_clf = VotingClassifier(estimators=[('KNN', best_models[\"KNN\"]), ('RF', best_models[\"RF\"]),\n",
    "                                              ('GBM', best_models[\"GBM\"])],\n",
    "                                  voting='soft').fit(X, y)\n",
    "    cv_results = cross_validate(voting_clf, X, y, cv=3, scoring=[\"accuracy\", \"f1\", \"roc_auc\"])\n",
    "    print(f\"Accuracy: {cv_results['test_accuracy'].mean()}\")\n",
    "    print(f\"F1Score: {cv_results['test_f1'].mean()}\")\n",
    "    print(f\"ROC_AUC: {cv_results['test_roc_auc'].mean()}\")\n",
    "    return voting_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data\"\n",
    "    column_names = [f\"attribute_{i}\" for i in range(1, 58)] + [\"label\"]\n",
    "    df = pd.read_csv(url, header=None, names=column_names)\n",
    "    X, y = data_prep(df)\n",
    "    base_models(X, y)\n",
    "    best_models = hyperparameter_optimization(X, y)\n",
    "    voting_clf = voting_classifier(best_models, X, y)\n",
    "    joblib.dump(voting_clf, \"voting_clf.pkl\")\n",
    "    return best_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "İşlem başladı\n",
      "Base Models....\n",
      "roc_auc: 0.954 (LR) \n",
      "roc_auc: 0.9243 (KNN) \n",
      "roc_auc: 0.9611 (SVC) \n",
      "roc_auc: 0.8734 (CART) \n",
      "roc_auc: 0.9716 (RF) \n",
      "roc_auc: 0.9616 (Adaboost) \n",
      "roc_auc: 0.974 (GBM) \n",
      "[LightGBM] [Info] Number of positive: 1208, number of negative: 1859\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001838 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6291\n",
      "[LightGBM] [Info] Number of data points in the train set: 3067, number of used features: 57\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.393870 -> initscore=-0.431073\n",
      "[LightGBM] [Info] Start training from score -0.431073\n",
      "[LightGBM] [Info] Number of positive: 1209, number of negative: 1858\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001743 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6366\n",
      "[LightGBM] [Info] Number of data points in the train set: 3067, number of used features: 57\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.394196 -> initscore=-0.429707\n",
      "[LightGBM] [Info] Start training from score -0.429707\n",
      "[LightGBM] [Info] Number of positive: 1209, number of negative: 1859\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001656 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6468\n",
      "[LightGBM] [Info] Number of data points in the train set: 3068, number of used features: 57\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.394068 -> initscore=-0.430245\n",
      "[LightGBM] [Info] Start training from score -0.430245\n",
      "roc_auc: 0.9763 (LightGBM) \n",
      "Hyperparameter Optimization....\n",
      "########## KNN ##########\n",
      "roc_auc (Before): 0.9243\n",
      "roc_auc (After): 0.929\n",
      "KNN best params: {'n_neighbors': 7}\n",
      "\n",
      "########## CART ##########\n",
      "roc_auc (Before): 0.8696\n",
      "roc_auc (After): 0.9379\n",
      "CART best params: {'ccp_alpha': 0, 'min_samples_leaf': 20, 'min_samples_split': 4}\n",
      "\n",
      "########## RF ##########\n",
      "roc_auc (Before): 0.971\n",
      "roc_auc (After): 0.9715\n",
      "RF best params: {'max_features': 'log2', 'min_samples_leaf': 5, 'n_estimators': 500}\n",
      "\n",
      "########## GBM ##########\n",
      "roc_auc (Before): 0.974\n",
      "roc_auc (After): 0.9752\n",
      "GBM best params: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200}\n",
      "\n",
      "Voting Classifier...\n",
      "Accuracy: 0.9317462585398504\n",
      "F1Score: 0.9124940773269822\n",
      "ROC_AUC: 0.9734486594909052\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"İşlem başladı\")\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
